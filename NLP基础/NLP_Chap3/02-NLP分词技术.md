# NLP分词技术

## 什么是分词？

在语言理解中，词是最小的能够独立活动的有意义的语言成分。**分词是NLP的基础任务**，将句子、段落和文章这种长文本，分解成以字词为单位的数据结构，方便后续的处理分析工作。

## 为什么要分词？

* **将复杂问题转化为数学问题**

  机器学习之所以看上去可以解决很多复杂的问题，是因为它把这些问题都转化为了数学问题。而 NLP 也是相同的思路，文本都是一些**「非结构化数据」**，我们需要先将这些数据转化为**「结构化数据」**，结构化数据就可以转化为数学问题了，而分词就是转化的第一步。

* **词是一个比较合适的粒度**

  **词**是表达完整含义的最小单位。字的粒度太小，无法表达完整含义，比如”鼠“可以是”老鼠“，也可以是”鼠标“。而句子的粒度太大，承载的信息量多，很难复用。比如”传统方法要分词，一个重要原因是传统方法对远距离依赖的建模能力较弱。”

## 中文分词和英文分词的区别

* **区别1：分词方式不同，中文更难**

  英文有天然的空格作为分隔符，但是中文没有。所以如何切分是一个难点，再加上中文里一词多意的情况非常多，导致很容易出现歧义。下文中难点部分会详细说明。

* **区别2：英文单词有多种形态**

  英文单词存在丰富的变形变换。为了应对这些复杂的变换，英文NLP相比中文存在一些独特的处理步骤，我们称为词形还原（Lemmatization）和词干提取（Stemming）。

  **词性还原：**does，done，doing，did 需要通过词性还原恢复成 do。

  **词干提取：**cities，children，teeth 这些复数形式，需要转换为 city，child，tooth这些基本形态

* **区别3：中文分词需要考虑粒度问题**

  例如：「中国科学技术大学」就有很多种分法：

  <center>
  中国科学技术大学<br/>
  中国\科学技术\大学<br/>
  中国\科学\技术\大学<br/>
  </center>

  **粒度越大，表达的意思就越准确，但是也会导致召回比较少。所以中文需要不同的场景和要求选择不同的粒度。这个在英文中是没有的。**

## NLP分词难点

NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。说简单是因为分词的算法研究已经很成熟了，大部分的准确率都可以达到95%以上，说复杂是因为剩下的5%很难有突破，主要因为三点：

1. **粒度**，不同应用对粒度的要求不一样，比如“苹果手机”可以是一个词也可以是两个词。
2. **歧义**，比如：“结婚的和尚未结婚的”可以分成“结婚的/和/尚未结婚的”，也可以理解为“结婚的/和尚/为结婚的”。这样就产生了歧义。
3. **未登录词**，比如“skrrr”、“打call”等新兴词语。

## 两种典型的分词方法

### **一、基于规则分词**

**基于规则的分词**是一种机械分词方法，主要是通过**维护词典**，在切分语句时，将语句的每个字符串与词典（表）中的词进行逐一匹配，找到匹配则进行切分，否则不予切分。对于中文分词，基本思想是基于词典匹配，将待分词的中文文本根据一定规则切分和调整，然后跟词典中的词语进行匹配，匹配成功则按照词典的词分词，匹配失败通过调整或者重新选择，如此反复循环即可。代表方法有基于**正向最大匹配法**和基于**逆向最大匹配法**及**双向匹配法**。

* **优点：**速度快、成本低
* **缺点：**适应性不强，不同领域效果差异大。词典维护的成本较高，需要不断更新。

#### 1.正向最大匹配法（Maximum Match Method，MM法）

* **基本思想：**假定分词词典中的最长词有$i$个汉字字符，则用被处理文档的当前字串中的前$i$个作为匹配字段，查找字典。若字典中存在这样的一个$i$字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个$i$字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串进行重新匹配处理。如此进行下去，直到匹配成功，即切分出一个词或剩余字串长度为0为止。这样就完成的一轮匹配，然后取下一个$i$字字串进行匹配处理，直至整个文档被扫描完毕。

* **算法步骤描述：**

  1) 从左向右取待切分的语句的$m$个字符作为匹配字段，$m$为**机器词典**中**最长**词条的字符数。
  2) 查找机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。

* **Code & Example**

  ![MMcode](/Users/zhangyufeng/Documents/Typora Documents/MMcode.png)

  **结果为：**['研究生----', '命----', '的----', '起源----']

  看起来效果不是很好......容易被歧义影响，这时候就可以考虑使用**逆向最大匹配法（RMM）**进行处理。

#### 2.逆向最大匹配法（Reverse Maximum Match Method，RMM法）

* **基本思想：**从被处理文档的末端开始匹配扫描，每次取最末端的$i$个字符($i$为词典中最长词数)作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。
* **算法步骤描述：**
  1) 从右向左取待切分的语句的$m$个字符作为匹配字段，$m$为**机器词典**中**最长**词条的字符数。
  2) 查找机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的开头一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。
* **Code & Example**

![RMMcode](/Users/zhangyufeng/Documents/Typora Documents/RMMcode.png)

**结果为：**['研究----', '生命----', '的----', '起源----']

可以发现，使用逆向最大匹配法得到的结果就靠谱一些。其实，统计结果表明：单纯使用正向最大匹配的错误率约为$\frac{1}{169}$，单纯使用逆向最大匹配的错误率约为$\frac{1}{245}$。

#### 3.双向最大匹配法（Bi-direction Matching Method，BidM法）

* **基本思想：**将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配规则，**选取词数切分少的作为结果**。

* **算法描述：**

  1.对被分词的文本使用正向最大匹配法

  2.对被分词的文本使用逆向最大匹配法

  3.两个结果进行对比，**选取单字少的作为结果**

* **Code & Example**

![BidMcode](/Users/zhangyufeng/Documents/Typora Documents/BidMcode.png)

**结果为：**['研究----', '生命----', '的----', '起源----']

**总结：**基于规则的分词，一般都较为简单高效，但是词典的维护是一个很庞大的工程。在网络发达的今天，网络新词层出不穷，很难通过词典覆盖到所有词。

### 二、基于统计分词

随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的分词算法渐渐成为主流。其**主要思想**是把每个词看做是由词的最小单位的各个字组成的，如果相连的字在不同的文本中出现的次数越多，就证明这相连的字很可能就是一个词。因此我们就可以利用字与字相邻出现的频率来反应成词的可靠度，统计语料中相邻共现的各个字的组合的频度，当组合频度高于某一个临界值时，我们便可认为此字组可能会构成一个词语 。常用统计学习方法：**隐含马尔可夫（HMM）、条件随机场（CRF）、支持向量机（SVM）、深度学习**等算法。

基于统计的分词， 一般要做如下两步操作:

* 建立统计语言模型
* 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式

**优点：**适应性较强

**缺点：**成本较高，速度较慢

HMM、CRF、SVM等将单独在后面详细介绍。

**常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。**

### 三、常见中英文分词包

* 中文：[Hanlp](https://link.zhihu.com/?target=https%3A//github.com/hankcs/HanLP)、[Stanford 分词](https://link.zhihu.com/?target=https%3A//github.com/stanfordnlp/CoreNLP)、[ansj 分词器](https://link.zhihu.com/?target=https%3A//github.com/NLPchina/ansj_seg)、[哈工大 LTP](https://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp)、[jieba](https://link.zhihu.com/?target=https%3A//github.com/yanyiwu/cppjieba)
* 英文：[Keras](https://link.zhihu.com/?target=https%3A//github.com/keras-team/keras)、[Spacy](https://link.zhihu.com/?target=https%3A//github.com/explosion/spaCy)、[NLTK](https://link.zhihu.com/?target=https%3A//github.com/nltk/nltk)

## 参考资料

[1]https://zhuanlan.zhihu.com/p/50444885

[2]https://zhuanlan.zhihu.com/p/77281678

[3]《Python自然语言处理实战：核心技术与算法》